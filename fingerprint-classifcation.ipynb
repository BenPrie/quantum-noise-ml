{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:49:15.912893400Z",
     "start_time": "2024-06-03T11:49:11.173248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports, as always...\n",
    "from os import listdir, makedirs, path\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Random seeds.\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:03.608321800Z",
     "start_time": "2024-06-03T11:50:03.592633600Z"
    }
   },
   "id": "ff0562c78ddefbc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fingerprint Classification\n",
    "\n",
    "This notebook aims to replicate and advance the approaches to classification in [Martina et al. (2021)](https://arxiv.org/pdf/2109.11405), in which an SVM is used to binarily classify whether a given classical measurement was or was not produced by a given quantum circuit.\n",
    "\n",
    "This is a simple task which, when taken together with their *very* small circuit, yields for near perfect accuracy. Here, we will complicate things to push the capability of the models to investigate more thoroughly what may or may not be possible with regard to classifying the membership of a quantum state to a quantum device by its \"noise fingerprint\". \n",
    "\n",
    "The ideas fitting into the work of this notebook are as follows:\n",
    "- *Multi-class classification*. Using the data produced by Martina et al. (2021), can we present a multi-class prediction model that is not given any bias towards any particular model -- given a measurement of a quantum state, which device produced it?\n",
    "- *Larger/deeper circuits*. How does the performance degrade as the number of qubits increases, or as the circuit depth increases?\n",
    "- *Noise severity analysis*. Under which severities/forms of noise is performance best? Ideally, we can produce a visualisation of performance (e.g. accuracy) vs. noise intensity/severity. We might expect poor performance with little/no noise (not enough distinguishing information between membership classes), good performance with moderate noise, then poor performance again with large amounts of noise (too much randomness).\n",
    "\n",
    "For clarity, 'membership to a quantum device' in this context refers to 'being produced by that device'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63461cca007c2f91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Martina et al. (2021)'s Dataset\n",
    "\n",
    "Only the raw data is given, and the `createDataset.py` and `extractExecuction.py` scripts are abhorrent messes and crimes against humanity, so I'll do my best to re-create the \"extracting\" and \"creating\" process for a dataset from the data on their [GitHub](https://github.com/trianam/learningQuantumNoiseFingerprint/tree/main)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f89fcfc949817c9c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ibmq_athens': 750,\n 'ibmq_casablanca': 500,\n 'ibmq_lima': 250,\n 'ibmq_quito': 250,\n 'ibmq_santiago': 250,\n 'ibmq_5_yorktown': 250}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List all files in the \"walker\" directory.\n",
    "file_list = listdir('./martina/data/walker')\n",
    "\n",
    "# List of machines.\n",
    "# Note: IBM Bogota's files cannot be read -- ALL lead to pickle underflow.\n",
    "machines = ['ibmq_athens', 'ibmq_casablanca', 'ibmq_lima', 'ibmq_quito', 'ibmq_santiago', 'ibmq_5_yorktown']\n",
    "\n",
    "# How many files does each machine have.\n",
    "counts = {machine : 0 for machine in machines}\n",
    "for file in file_list:\n",
    "    for machine in machines:\n",
    "        if machine in file:\n",
    "            counts[machine] += 1\n",
    "\n",
    "# Any count above 250 includes custom splits -- we're not too interested in those.\n",
    "display(counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:09.046599700Z",
     "start_time": "2024-06-03T11:50:09.028770600Z"
    }
   },
   "id": "58024dbd026b791e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting\n",
    "\n",
    "This is the process of reading the stored data and translating it into probability distributions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b449e362e9739136"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# File paths.\n",
    "base_path = './martina/data/walker'\n",
    "extracted_path = './martina/data/walkerExtracted'\n",
    "    \n",
    "# Generate the output path.\n",
    "makedirs(extracted_path, exist_ok=True)\n",
    "\n",
    "# \"Window sizes\".\n",
    "ks = [1000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:12.106583700Z",
     "start_time": "2024-06-03T11:50:12.089577100Z"
    }
   },
   "id": "45a9056131e4159d"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Helper function to filter the file list into the (non-split) files of only a given machine.\n",
    "def filter_to_machine(file_list, machine):\n",
    "    # List of words that specify different types of data (e.g. split).\n",
    "    no_words = ['split', 'bis']\n",
    "    \n",
    "    return filter(\n",
    "        lambda file : machine in file and not (any([word in file for word in no_words])), file_list\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:29:46.087074500Z",
     "start_time": "2024-06-02T18:29:46.079546400Z"
    }
   },
   "id": "e0305231f0f04845"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "Extracting:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9d04512089b4e639b16184393eb76d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc5a8fb9b7b048b3b8b2189750f91c08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42da6175f0f044449dcf9588b4863743"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18574ef9cfdc45baa88cf334bddc036d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98f40dc72e1e457a879d0417cd3e88fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d055109e655848aea56ba643825a0bae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34d0dc6d28854edbb39f9cacfed37116"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbd6d796cfca424980c3920b3708b782"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c288882ca60470496051ddb191bfa03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f08495b0c7a24598ae46d4693536191c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e949bff64ee94bf1a58f40e3f3d35db9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3911af4d60cb46858cb08c36ad8399a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3795242162f46b5b9948b4a42cb0035"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting executions for each machine.\n",
    "for machine in tqdm(machines, desc='Extracting'):\n",
    "    executions = []\n",
    "    \n",
    "    # For each file belonging to the current machine.\n",
    "    for file in tqdm(filter_to_machine(file_list, machine), desc='Reading files', total=250):\n",
    "        # Read the contents of the file.\n",
    "        contents = pickle.load(open(path.join(base_path, file), 'rb'))\n",
    "        \n",
    "        # For each run of the circuit (of which there are 8000 in the Martina paper and data).\n",
    "        for n in range(len(contents['results'][0]['data']['memory'])):\n",
    "            current_execution = []\n",
    "            \n",
    "            # Note: we will not be doing repeated measures, nor will we \"read all bits\".\n",
    "            \n",
    "            # For each measurement step t (of which there are 9 in the Martina paper and data).\n",
    "            for t in range(len(contents['results'])):\n",
    "                execution = int(contents['results'][t]['data']['memory'][n], 0)\n",
    "                current_execution.append(execution)\n",
    "\n",
    "            executions.append(current_execution)\n",
    "            \n",
    "    # Cast to numpy array.\n",
    "    executions = np.array(executions)\n",
    "    \n",
    "    # Save the full executions of this machine. \n",
    "    np.savetxt(path.join(extracted_path, f'{machine}-executions.csv'), executions)\n",
    "    \n",
    "    # Break the executions into windows to be saved.\n",
    "    for k in ks:\n",
    "        # The window size must cleanly divide the number of executions.\n",
    "        if executions.shape[0] % k != 0: raise(Exception('Indivisible by window size.'))\n",
    "        \n",
    "        # Initialise probabilities array.\n",
    "        probs = np.zeros(shape=(\n",
    "            executions.shape[0] // k, executions.shape[1], np.unique(executions).shape[0]\n",
    "        ), dtype=np.float32)\n",
    "        \n",
    "        # Calculate probabilities with the given window size.\n",
    "        for n in tqdm(range(executions.shape[0]), desc='Calculating probabilities'):\n",
    "            i = n // k\n",
    "            \n",
    "            for t in range(executions.shape[1]):\n",
    "                probs[i, t, executions[n, t]] += 1\n",
    "                \n",
    "        probs = probs / k\n",
    "        \n",
    "        # Save the window.\n",
    "        np.save(path.join(extracted_path, f'{machine}-probabilities-{k}.npy'), probs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:38:04.965593100Z",
     "start_time": "2024-06-02T18:29:53.643945100Z"
    }
   },
   "id": "e85f4453bd09e9cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating\n",
    "\n",
    "Now we arrange the extracted run statistics into a dataset. For the sake of ease of use, we'll chuck all that into a PyTorch Dataset and then Dataloader."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e462c834653e4756"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# File paths.\n",
    "dataset_path = './martina/data/walkerDataset'\n",
    "makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Specify the window size.\n",
    "k = ks[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:19.747446200Z",
     "start_time": "2024-06-03T11:50:19.728903900Z"
    }
   },
   "id": "995cf416e51dec3e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Pack all the probability distributions (from all machines) into an array.\n",
    "probs = [np.load(path.join(extracted_path, f'{machine}-probabilities-{k}.npy')) for machine in machines]\n",
    "order = [np.arange(prob.shape[0]) for prob in probs]\n",
    "\n",
    "# Features (x), labels (y) format.\n",
    "xs, ys = [], []\n",
    "for i in range(min(map(len, order))):\n",
    "    for p in range(len(probs)):\n",
    "        xs.append(probs[p][order[p][i]])\n",
    "        ys.append(p)\n",
    "        \n",
    "# Numpify those arrays.\n",
    "xs = np.array(xs, dtype=np.float32)\n",
    "ys = np.array(ys, dtype=np.float32)\n",
    "\n",
    "# Save in this format.\n",
    "np.savez_compressed(path.join(dataset_path, f'all-dataset-{k}'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:30.827610700Z",
     "start_time": "2024-06-03T11:50:30.770317900Z"
    }
   },
   "id": "269dba5e19a8fd6e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# PyTorch Dataset class for handling these data.\n",
    "class PyTorchDataset(Dataset):\n",
    "    def __init__(self, xs : np.array, ys : np.array):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        \n",
    "    def __len__(self):\n",
    "        return ys.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return xs[idx], ys[idx]\n",
    "    \n",
    "# Train-test-validation splitting the data.\n",
    "test_size, val_size = 0.25, 0.25\n",
    "xs_train, xs_test, ys_train, ys_test = train_test_split(xs, ys, test_size=test_size, shuffle=False)\n",
    "xs_train, xs_val, ys_train, ys_val = train_test_split(xs_train, ys_train, test_size=val_size, shuffle=False)\n",
    "\n",
    "# Create train-test-val Dataset objects.\n",
    "train_dataset = PyTorchDataset(xs_train, ys_train)\n",
    "val_dataset = PyTorchDataset(xs_val, ys_val)\n",
    "test_dataset = PyTorchDataset(xs_test, ys_test)\n",
    "\n",
    "# And create train-test-val Dataloader objects.\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:33.125588Z",
     "start_time": "2024-06-03T11:50:33.109696Z"
    }
   },
   "id": "184435ac64616bff"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0.464, 0.53 , 0.002, 0.004],\n        [0.236, 0.253, 0.247, 0.264],\n        [0.076, 0.466, 0.204, 0.254],\n        [0.153, 0.446, 0.159, 0.242],\n        [0.139, 0.299, 0.189, 0.373],\n        [0.251, 0.205, 0.221, 0.323],\n        [0.297, 0.203, 0.33 , 0.17 ],\n        [0.396, 0.23 , 0.223, 0.151],\n        [0.377, 0.271, 0.206, 0.146]], dtype=float32),\n 0.0)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.\n",
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T11:50:36.513485Z",
     "start_time": "2024-06-03T11:50:36.498576300Z"
    }
   },
   "id": "4328aa1d910d0945"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "Given the measurement of a quantum state, what is the probability distribution over the set of devices (for the likelihood of membership), and subsequently which device is most likely to have produced the state?\n",
    "\n",
    "The paper has two considerations for the input: (1) considering measurement outcomes at a single measurement step $k\\in[1,\\dots,9]$; and (2) concatenating all measurement outcomes in an ordered series $1,\\dots,k$. Since the circuit is supposedly regenerated and then measured independently for each measurement step $k$, the only tangible difference between them is *time*. This is the point, actually -- the elements of the ordered series are independent of one another, yet ordered in time. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c669d22f09703309"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single-measurement\n",
    "\n",
    "We'll define a model then train it to classify a given probability distribution as belonging to one of the machines. We'll train on each of the $k$ to obtain $k$ models that have learnt to discriminate distributions by machine membership at each time step. Then we can use each of these models to test on each time step too, giving that we will yield $k$ accuracy scores per model -- how well does each model discriminate at a given time step, considering the time step it was trained in. \n",
    "\n",
    "If we find that models are only really capable of classifying in or very near the time step they were trained in, then we are discovering that time-ordered series of noise are necessary to distinguish machines. This was the broad conclusion of the paper, but their method was not as direct as this (for whatever reason). On the other hand, having a model trained at the $k$-th step being able to classify quite well at the ($k+k')$-th for some $k'\\gg0$ tells us that there is enough information in a slice of time to understand the inherent difference between the noise being produced by machines -- time-dependent or otherwise.\n",
    "\n",
    "Another thing that might be interesting is to train a model with *all* data with no regard to time. This may be considered a brute-force version of the aforementioned test for time-dependence, but I think this might lead to a model capable of doing well at time steps it has never seen. For the sake of this last point, we might like to only train on the first few time steps (say 5) and evaluate on the final time step (i.e. 9) to allow for some distance between train and test sets.\n",
    "\n",
    "Finally, we may also find interesting resutls training a model to discriminate *the time step* rather than the machine. That is, given a probability distribution from a single machine, can we classify the time step in which it was generated. We might then find use for this classify as a \"selector\" of sorts for choosing better architectural choice later on (e.g. if we have $k$ different classifiers for each time step as suggested above, then this model could be used to decide which to use given an unknown state)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cd9b090f3ae45e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dcae3b92b608bee1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
