{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:43:04.842504500Z",
     "start_time": "2024-06-02T14:43:04.835499500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports, as always...\n",
    "from os import listdir, makedirs, path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random seed.\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff0562c78ddefbc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fingerprint Classification\n",
    "\n",
    "This notebook aims to replicate and advance the approaches to classification in [Martina et al. (2021)](https://arxiv.org/pdf/2109.11405), in which an SVM is used to binarily classify whether a given classical measurement was or was not produced by a given quantum circuit.\n",
    "\n",
    "This is a simple task which, when taken together with their *very* small circuit, yields for near perfect accuracy. Here, we will complicate things to push the capability of the models to investigate more thoroughly what may or may not be possible with regard to classifying the membership of a quantum state to a quantum device by its \"noise fingerprint\". \n",
    "\n",
    "The ideas fitting into the work of this notebook are as follows:\n",
    "- *Multi-class classification*. Using the data produced by Martina et al. (2021), can we present a multi-class prediction model that is not given any bias towards any particular model -- given a measurement of a quantum state, which device produced it?\n",
    "- *Larger/deeper circuits*. How does the performance degrade as the number of qubits increases, or as the circuit depth increases?\n",
    "- *Noise severity analysis*. Under which severities/forms of noise is performance best? Ideally, we can produce a visualisation of performance (e.g. accuracy) vs. noise intensity/severity. We might expect poor performance with little/no noise (not enough distinguishing information between membership classes), good performance with moderate noise, then poor performance again with large amounts of noise (too much randomness).\n",
    "\n",
    "For clarity, 'membership to a quantum device' in this context refers to 'being produced by that device'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63461cca007c2f91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Martina et al. (2021)'s Dataset\n",
    "\n",
    "Only the raw data is given, and the `createDataset.py` and `extractExecuction.py` scripts are abhorrent messes and crimes against humanity, so I'll do my best to re-create the \"extracting\" and \"creating\" process for a dataset from the data on their [GitHub](https://github.com/trianam/learningQuantumNoiseFingerprint/tree/main)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f89fcfc949817c9c"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ibmq_athens': 750,\n 'ibmq_casablanca': 500,\n 'ibmq_lima': 250,\n 'ibmq_quito': 250,\n 'ibmq_santiago': 250,\n 'ibmq_5_yorktown': 250}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List all files in the \"walker\" directory.\n",
    "file_list = listdir('./martina/data/walker')\n",
    "\n",
    "# List of machines.\n",
    "# Note: IBM Bogota's files cannot be read -- ALL lead to pickle underflow.\n",
    "machines = ['ibmq_athens', 'ibmq_casablanca', 'ibmq_lima', 'ibmq_quito', 'ibmq_santiago', 'ibmq_5_yorktown']\n",
    "\n",
    "# How many files does each machine have.\n",
    "counts = {machine : 0 for machine in machines}\n",
    "for file in file_list:\n",
    "    for machine in machines:\n",
    "        if machine in file:\n",
    "            counts[machine] += 1\n",
    "\n",
    "# Any count above 250 includes custom splits -- we're not too interested in those.\n",
    "display(counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:19:22.938288800Z",
     "start_time": "2024-06-02T18:19:22.926858200Z"
    }
   },
   "id": "58024dbd026b791e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting\n",
    "\n",
    "This is the process of reading the stored data and translating it into probability distributions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b449e362e9739136"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# File paths.\n",
    "base_path = './martina/data/walker'\n",
    "extracted_path = './martina/data/walkerExtracted'\n",
    "    \n",
    "# Generate the output path.\n",
    "makedirs(extracted_path, exist_ok=True)\n",
    "\n",
    "# \"Window sizes\".\n",
    "ks = [1000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T23:24:56.488366200Z",
     "start_time": "2024-06-02T23:24:56.479215800Z"
    }
   },
   "id": "45a9056131e4159d"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Helper function to filter the file list into the (non-split) files of only a given machine.\n",
    "def filter_to_machine(file_list, machine):\n",
    "    # List of words that specify different types of data (e.g. split).\n",
    "    no_words = ['split', 'bis']\n",
    "    \n",
    "    return filter(\n",
    "        lambda file : machine in file and not (any([word in file for word in no_words])), file_list\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:29:46.087074500Z",
     "start_time": "2024-06-02T18:29:46.079546400Z"
    }
   },
   "id": "e0305231f0f04845"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "Extracting:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9d04512089b4e639b16184393eb76d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc5a8fb9b7b048b3b8b2189750f91c08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42da6175f0f044449dcf9588b4863743"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18574ef9cfdc45baa88cf334bddc036d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98f40dc72e1e457a879d0417cd3e88fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d055109e655848aea56ba643825a0bae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34d0dc6d28854edbb39f9cacfed37116"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbd6d796cfca424980c3920b3708b782"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c288882ca60470496051ddb191bfa03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f08495b0c7a24598ae46d4693536191c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e949bff64ee94bf1a58f40e3f3d35db9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Reading files:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3911af4d60cb46858cb08c36ad8399a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating probabilities:   0%|          | 0/2000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3795242162f46b5b9948b4a42cb0035"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting executions for each machine.\n",
    "for machine in tqdm(machines, desc='Extracting'):\n",
    "    executions = []\n",
    "    \n",
    "    # For each file belonging to the current machine.\n",
    "    for file in tqdm(filter_to_machine(file_list, machine), desc='Reading files', total=250):\n",
    "        # Read the contents of the file.\n",
    "        contents = pickle.load(open(path.join(base_path, file), 'rb'))\n",
    "        \n",
    "        # For each run of the circuit (of which there are 8000 in the Martina paper and data).\n",
    "        for n in range(len(contents['results'][0]['data']['memory'])):\n",
    "            current_execution = []\n",
    "            \n",
    "            # Note: we will not be doing repeated measures, nor will we \"read all bits\".\n",
    "            \n",
    "            # For each measurement step t (of which there are 9 in the Martina paper and data).\n",
    "            for t in range(len(contents['results'])):\n",
    "                execution = int(contents['results'][t]['data']['memory'][n], 0)\n",
    "                current_execution.append(execution)\n",
    "\n",
    "            executions.append(current_execution)\n",
    "            \n",
    "    # Cast to numpy array.\n",
    "    executions = np.array(executions)\n",
    "    \n",
    "    # Save the full executions of this machine. \n",
    "    np.savetxt(path.join(extracted_path, f'{machine}-executions.csv'), executions)\n",
    "    \n",
    "    # Break the executions into windows to be saved.\n",
    "    for k in ks:\n",
    "        # The window size must cleanly divide the number of executions.\n",
    "        if executions.shape[0] % k != 0: raise(Exception('Indivisible by window size.'))\n",
    "        \n",
    "        # Initialise probabilities array.\n",
    "        probs = np.zeros(shape=(\n",
    "            executions.shape[0] // k, executions.shape[1], np.unique(executions).shape[0]\n",
    "        ), dtype=np.float32)\n",
    "        \n",
    "        # Calculate probabilities with the given window size.\n",
    "        for n in tqdm(range(executions.shape[0]), desc='Calculating probabilities'):\n",
    "            i = n // k\n",
    "            \n",
    "            for t in range(executions.shape[1]):\n",
    "                probs[i, t, executions[n, t]] += 1\n",
    "                \n",
    "        probs = probs / k\n",
    "        \n",
    "        # Save the window.\n",
    "        np.save(path.join(extracted_path, f'{machine}-probabilities-{k}.npy'), probs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T18:38:04.965593100Z",
     "start_time": "2024-06-02T18:29:53.643945100Z"
    }
   },
   "id": "e85f4453bd09e9cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating\n",
    "\n",
    "Now we arrange the extracted run statistics into datasets. Unlike the original work, we will do this to work nicely with PyTorch frameworks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e462c834653e4756"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# File paths.\n",
    "dataset_path = './martina/data/walkerDataset'\n",
    "makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Specify the window size.\n",
    "k = ks[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T23:25:03.924714700Z",
     "start_time": "2024-06-02T23:25:03.918771400Z"
    }
   },
   "id": "995cf416e51dec3e"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "# Pack all the probability distributions (from all machines) into an array.\n",
    "probs = [np.load(path.join(extracted_path, f'{machine}-probabilities-{k}.npy')) for machine in machines]\n",
    "order = [np.arange(prob.shape[0]) for prob in probs]\n",
    "\n",
    "# Features (x), labels (y) format.\n",
    "xs, ys = [], []\n",
    "for i in range(min(map(len, order))):\n",
    "    for p in range(len(probs)):\n",
    "        xs.append(probs[p][order[p][i]])\n",
    "        ys.append(p)\n",
    "        \n",
    "# Numpify those arrays.\n",
    "xs = np.array(xs, dtype=np.float32)\n",
    "ys = np.array(ys, dtype=np.float32)\n",
    "\n",
    "# Save in this format.\n",
    "np.savez_compressed(path.join(dataset_path, f'all-dataset-{k}'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T23:39:07.213631300Z",
     "start_time": "2024-06-02T23:39:07.195658100Z"
    }
   },
   "id": "269dba5e19a8fd6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "184435ac64616bff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "Given the measurement of a quantum state, what is the probability distribution over the set of devices (for the likelihood of membership), and subsequently which device is most likely to have produced the state?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c669d22f09703309"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dcae3b92b608bee1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
